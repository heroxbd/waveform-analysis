\documentclass[12pt]{article}
\usepackage{setspace} % for \onehalfspacing and \singlespacing macros
\onehalfspacing 
\usepackage{amsmath}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\par\singlespacing\small\itshape}


\title{Reply to report on JINST\_063P\_1221}
\date{April 29, 2022}

\begin{document}
\maketitle
Dear Editor,

We thank the referees for their critical comments.  They pointed out the key points to improve the paper into a clearer and more useful form to the community.

\begin{quote}
This paper is a survey of various methods for fitting and extracting optical photon intensity versus time from a photo-sensor/photo-multiplier waveform. The paper has a number of interesting points and some novel ideas, however I do not recommend publication in the present form.

The paper has a number of issues from conceptual to detail that need to be worked out. Most importantly, there is no central theme to the paper despite the title that claims to be searching for the ultimate waveform analysis tool. To have this claim there needs to be a detailed analysis of a single preferred tool that shows its function for a variety of circumstances and not just the idealized circumstance.

While inferring PEs from a waveform is a laudable (and a very ambitious) goal, the performance needed depends on the light source type, the geometry of the source (detector type), as well as the physics requirements. None of the requirements are truly examined in the paper.
\end{quote}

This is the biggest issue that confuses our referee.  We centered our research theme at large liquid neutrino and dark matter experiments, but the big picture of experimental setup is not clear.

We modify the title to be more specific:

Towards the ultimate waveform analysis for neutrino and dark matter detectors

We added a section to describe a typical large liquid neutrino or dark matter detector.  It clarifies which output of the PMTs are the most important for this scenario.

\begin{quote}
Clarity: because of the very ambitious goal, the authors have examined numerous algorithms and methods. Each of these comes with its own jargon and complicated syntax. The result is that paper lacks clarity. It is just impossible in the end to tell what variable has a hat and which one has a twiddle, etc. There is a lot of unnecessary jargon that must be coming from the world of advanced statistics and machine language. I cannot tell why something is ``heuristic'' and the rest is not. What is ``posterior charge distribution'' ? I very much suggest eliminating all such jargon and write in plain language.
\end{quote}

We eliminated unecessary symbols and compiled a dictionary in front to aid the readers.  We defined ``heuristic'' as methods based on intuition with minimal assumptions of the instrumental and statistical assumptions in the text, and eliminated ``posterior charge distribution'' jargons.

\begin{quote}
Assumptions: there are some errors in the assumptions regarding the operation of the photo-sensor or phototube as an instrument.

Lack of data: the paper is entirely based on simulations level analysis. If the authors were to apply the best selected method to a particular data type from a real detector or instrument, it would add considerable value.
\end{quote}

The focus of this paper is on comparing all the available waveform analysis algorithms known to us.  It is the central theme to score the algorithms in terms of PE time and counting accruracies, which is only possible with simulation data.

To employ real world data for the benchmark of, for example, particle identification, detector optical response needs to be included besides the waveform analysis.  This will complicate the comparison and enlengthen and manuscript.

The value of this study is to establish in an ideal case the algorithm of choice.  Building upon such an understanding, we will apply it to neutrino and dark matter experiments in the field, and report in subsequent papers for the real world performance improvements in energy resolution, event topology and particle momentum reconstructions, etc.

The best algorithm may well change when we migrate to the real experiments from the ideal assumptions.  But if that is the case, new insights will emerge.  We think this study stands on its own to make a simplified comparison with all the algorithms.

\begin{quote}

Detailed comments:

Line 90: Referee is surprised by the choice of using a lognormal PDF for description of a single PE waveform. This needs clarification given that the authors want to construct the ``ultimate waveform''.

First, it is well known that the risetime and falltime of a PMT comes from the RC constants in the equivalent (Thevenin) circuit of the PMT and cable. 

Secondly, the lognormal sufffers from two problems: it is only valid for the range $t>0$, and its parameters do not have a good relationship with theunderlying PMT equivalent circuit.

I very much suggest the authors seriously consider redoing this analysis with a distribution that has more physical meaning than the lognormal. 
\end{quote}

We regard the distribution only valid for t>0 a feature for modeling PE-waveform causality.  The tail of lognormal asympototically approaches exponential dacay shape of RC circuit. lognormal is not an ideal shape to use, but it is the most convenient form we know of.  We speculate our conclusions do not change substantially by using a different SER pulse shape.

We seriously studied the literature and added a paragraph to the manuscript to explain why we use lognormal.  We look forward to more reference suggestions from our referee if the survey is incomplete.

\begin{quote}
Line 95: This number 0.25 for the relative variance of the gain seems rather small. It would correspond to a first stage gain of 16. A much more reasonable gain variance is $\sim$ 0.4 which corresponds to first stage gain of $\sim$ 6. However this creates a problem for the model if the gain is assumed to fluctuate as a Gaussian. A much more reasonable assumption is a gamma distribution as is well known in the literature.
\end{quote}

Very good point, thank you!  Our assumptions were not realistic.  We work the simulation with a gamma distribution of charge with 0.4*0.4 relative gain variance, and update the texts and plots.  The conclusions do not change, implying that our benchmarks are robust. 

\begin{quote}
Line 129: ``compound Poisson distribution with a Gaussian jump''
\end{quote}

Updated.

\begin{quote}
Line 148-150: This example highly depends on the gain variance assumed. The gain variance assumed here (25\%) is quite small compared to most real PMTs.
\end{quote}

When the gain variance becomes larger, it argument here is more legitimate because 1, 2 and 3 PEs has more chances to produce identical charges. We change the variance of gain from 0.25 to 0.4 to match prevalent PMTs. Though the performance of waveform analysis methods depends on the variance we choose, a larger gain spread leads to a larger chance to misidentify PE numbers, which further justifies our argument here.

\begin{quote}
Equation 3.2:

The term $\mu\phi(t-t_0)-\hat{\phi}(t)$ appears to be a normalization correction to the usual KL divergence formula. Could you please explain in the text ?
\end{quote}

Sure, we added an explanation in the text.

\begin{quote}
Please put $\log\mu$ in brackets to avoid confusion.
\end{quote}

Updated.

\begin{quote}
Line 185-186: waveform shifting: This is extremely confusing. What was actually done for this calculation ? What is the method. 
\end{quote}

We have added a paragraph to explain the motivation of waveform shifting.  Our purpose is to summarize and abstract from the common practice in the field. 

\begin{quote}
What waveform is being shifted ? What is $\Delta t$ ? How is it chosen according to single PE ?
\end{quote}

If we regard the waveform as a set of smeared PE time. It should be shifted by $\Delta t$ to minimize the wasserstain distance between one PE time and a PE pulse. We have reorganized this section to reflect your question, hopefully explaining it better.

\begin{quote}
Line 193: ``shift $\Delta t$ ...'' too much is left unexplained here. It is very confusing.

Section 3.3: This section introduction is thoroughly too brief and unhelpful. Deconvolution is known to be unreliable due to poor conditioning of the data. How is that handled for real analysis of data ?
\end{quote}

This is very true.  We don't think the devoncolution is suitable for waveform analysis, although many smoothing trick exist to mitigate the poor conditioning issue.  However there is a rising trend to use deconvolution in the neutrino-experiment community.

We want to make a objective comparison here to advise against using deconvolution, in addition to the theoretical arguments.  We updated the section introduction to make it clear.  Thanks!

\begin{quote}
Equation 3.10: what is $q_{th}$ ?
\end{quote}

We added a better explanation in the text and the table of symbols.

\begin{quote}
Equation 3.12: Too much notation is being introduced and not enough definitions. What is twiddle ? What is a smoothed ``w'' ? How was it smoothed ?
\end{quote}

These are truly confusing. We eliminated unecessary symbols and compiled a dictionary in front to aid the readers. 

\begin{quote}
Line 222: This line illustrated the issue with this paper. It is written in the style of class room notes. It needs to be deeply edited to focus only on the essential message and a single chain of logic that supports the conclusion.
\end{quote}

We modified Line 222 and other places to be more formal and concise.

\begin{quote}
Line 232: thoroughly confusing and undefined.

Line 234: As discussed in section 3.1.3 ... sparsity ... The issue of sparsity is not discussed in section 3.1.3.

Figure 7 caption, line 238, line 242: This whole section has many undefined concepts and issues. what is a ``kernel = 21'' ? what are number of channels ? The behavior of $D_W$ depends on the assumed parameters of the pulse. The statement on line 238 is stated as a general statement. It is not a general result. What is ``matching of waveforms horizontally'' ? What is effective $\hat{\alpha}$ scaling ?
\end{quote}

% TBD

\begin{quote}
Figure 8: This uses language that is probably unfamiliar to the readers of JINST. What is a ``hinge'' and what are ``whiskers'' ? We do not do 25\% and 75\% confidence intervals usually.
\end{quote}

Using boxplots is a confusing way. We change the ``hinge'' and what are ``whiskers'' to the quantile of Gaussian distribution's positive and negative standard deviation, which are 15.8\% and 84.1\%. This errorbar setting is widely used in describing non-Gaussian distribution.

\begin{quote}
Line 253: The referee has a hard time understanding why $N_{PE}$ needs to be left unknown. If $N_{PE}$ is large then the integral of the pulse divided by the mean gain should be a very close estimate of $N_{PE}$. 
\end{quote}

Yes! If NPE is large, all the methods have the same resolution as charge integration in figure 15(d).

\begin{quote}
If $N_{PE}$ is small the gain fluctuation will prevent a good estimate, but it is still reasonably known.
\end{quote}

In photon-counting mode it is reasonably known, but in section 2.3.2 and eq. (2.8) we show that there is (1 + Var[q]) room of improvement from integration.

Any small improvement in the precision of counting NPE directly converts to a boost in energy resolution. For example, an improvement of 10\% energy resolution will shorten the JUNO measurement of neutrino mass ordering from 6 years to 3 years.

\begin{quote}
Section 3.5.1: There some conceptual problems here. I do not see how RSS can work well with noise and with pulses that have a sparse tail of photons. I think this method is also too specific for a pulse that has relatively compact shape. For pulses that are long with sparse tails this will not work.
\end{quote}

With the grid $t'$, PE times are fixed and only $q'$ can be varied.  No PEs at some candidate $t_i'$ is represented by $q'=0$.  If the grid $t'$ covers all the PEs, it will work.  We added a sentense in the text to better explain it.

\begin{quote}
Fig. 9: The demonstration is just not generic enough. The pulse structure assumed is relatively compact with only a few photons and with good single PE resolution.
\end{quote}

No it is not generic, but it is typical in neutrino and dark matter experiments to have $\sim 4$ PEs in a PMT channel.  Some of them are even lower. The purpose is to plot different algorithms on the same waveform, as supplimentary illustrations to the formulae.

The 20-30 PE cases are covered later in the final figure.

That said, if you insist, we can put another waveform example with more PEs.

\begin{quote}
Eq. 3.17: As remarked earlier, the assumption of a normal distribution for the charge gain is wrong. And it creates a problem for the model since with a normal distribution the charge can be negative which is unphysical.
\end{quote}

Gaussian is not the best model to describe the charge distribution, especially when the variance is large. We change the Gaussian to Gamma distribution to avoid unphysical negative charges. The analysis methods are mostly not affected, because for a Gaussian with mean 1 and variance $0.4^2$, 99.4\% of the distribution is positive.

\begin{quote}
Line 238: I suggest not spending time in the paper on techniques that do not work well.
\end{quote}

On line 238 for CNN, $D_\mathrm{w}$ is a loss function, it stops increasing means it works well.  Actually in terms of $D_\mathrm{w}$ alone CNN out-performs FSMP.

\begin{quote}
Line 324: I very much suggest the authors rewrite this paper based on only the FSMP technique and provide evidence with data that it works well. And provide much more detail on its performance for a variety of conditions.
\end{quote}

On line 324 for the efficiency of MCMC, the MCMC is a critical step towards FSMP.  FSMP it built upon MCMC, only employing several clever tricks to deal with nuisance parameters, likelihoods and varying number
of variables.

The suggestion is well considered.  We shall follow up a paper on FSMP tested with data from different PMTs and various conditions, and also on the convergence, hyperparameters of FSMP.  This paper fits in the gap of all the undocumented algorithms in the wild, and sets the stage for the up coming one.

Our coverage of methods in this paper are based on 2 criteria:

\begin{itemize}
    \item widely used but poorly documented algorithms
    \item proficient algorithms
\end{itemize}

\begin{quote}
Line 362: ``the message is clear'' ? I do not think so. Fig 15b is very confusing. The ``ratio'' is undefined. 
\end{quote}

We give a clearer definition of ``ratio'' in the caption.

\begin{quote}
The performance of ``1st'' is only a few percent worse. 
\end{quote}

``1st'' is off the scale, it goes all the way to 2 for larger mu.  The plot area focuses around 1.00.

A few percent worse is significant for neutrino and dark matter experiments.  Even 0.1ns resolution difference converts to centimeters difference in positional precision.

\begin{quote}
Secondly, the result is inconsistent with the earlier Fig. 3. Only modest improvement is expected with waveform analysis and only under some circumstances according to Fig. 3.
\end{quote}

We have reformed the tonality around the text of Fig. 3 to ephasize the resolution losses by 1st-PE estimator.  Except only 2 special cases (exponential light curve or 0-1 count region), the improvement is far more significant (x2 difference) in most of the experimental setups.

\end{document}
